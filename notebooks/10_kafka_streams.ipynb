{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **10. Using Kafka Streams in Capymoa**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how a **KafkaStream** class can be used to get instances from Kafka Producers\n",
    "\n",
    "**Apache Kafka** is an open source distributed streaming system used for stream processing, real-time data pipelines, and data integration at scale. Continuously received data can be flexibly processed using Kafka.\n",
    "\n",
    "---\n",
    "\n",
    "*More information about CapyMOA can be found in* https://www.capymoa.org\n",
    "\n",
    "**last update on 08/11/2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### For Windows and for localhost servers\n",
    "\n",
    "- Skip this section if the server already exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Kafka for Windows\n",
    "\n",
    "- Head to the [**Apache Kafka** Website](https://kafka.apache.org/downloads) and download the `.tgz` tar file for **Windows**\n",
    "- Extract the downloaded `.tgz` file\n",
    "- **Recommended**: To set up the environment variables easily, move the extracted folder to the `C:\\ Drive` and rename it to `kafka`\n",
    "- Head to `kafka\\config\\server.properties` and change the `log.dirs=/tmp/kafka-logs` to `log.dirs=c:/kafka/kafka-logs` or the location and name of your installed **kafka** folder. Save this change.\n",
    "- Head to `kafka\\config\\zookeeper.properties` and change the `dataDirs=/tmp/zookeeper` to `dataDirs=c:/kafka/zookeeper-data` or the location and name of your installed **kafka** folder. Save this change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Setup Zookeeper\n",
    "\n",
    "- Open a new command line and head to the directory where **kafka** was installed, for e.g, `c:/kafka`\n",
    "- Run the command:\n",
    "\n",
    "`.\\bin\\windows\\zookeeper-server-start.bat .\\config\\zookeeper.properties`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Setup Local Server\n",
    "\n",
    "- Open a new command line and head to the directory where **kafka** was installed, for e.g, `c:/kafka`\n",
    "- Run the command:\n",
    "\n",
    "`.\\bin\\windows\\kafka-server-start.bat .\\config\\server.properties`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create a Kafka Topic\n",
    "\n",
    "- Open a new command line and head to the directory where **kafka** was installed, for e.g, `c:/kafka`\n",
    "- Head to `c:\\kafka\\bin\\windows`\n",
    "- Run the command:\n",
    "\n",
    "`.\\kafka-topics.bat --create --topic <topic_name> --bootstrap-server <server_name>`\n",
    "\n",
    "Note: For Local Servers the server name is usually `localhost:9092`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Run a Kafka Producer\n",
    "\n",
    "- In the same command line/location as where the topic is created (`c:\\kafka\\bin\\windows`) run the command:\n",
    "\n",
    "`.\\kafka-console-producer.bat --broker-list <server_name> --topic <topic_name>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Add Instances to the Kafka Producer\n",
    "\n",
    "Add JSON formatted data instanced to the kafka producer by typing instances after the prompt `>`\n",
    "\n",
    "Note: The Kafka Producer must have a JSON format - one line per instance\n",
    "For example:\n",
    "\n",
    "`{\"feature1\": 0.123456, \"feature2\": 0.828380, \"feature3\": 0.141512, \"feature4\": 0.314159, \"feature5\": 0.422915, \"feature6\": 0.414912, \"target/class\": 1}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a **KafkaStream** instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from capymoa.stream._stream import KafkaStream\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initiate the KafkaStream Class depending on the known configuration\n",
    "\n",
    "Options for\n",
    "- \"Classification\" or \"Regression\" (mention the `target_type`)\n",
    "- Schema or No Schema\n",
    "- Data Types or Unknown Data Types \\\n",
    "FORMAT for Data Types: `[('column1', np.float64), ('column2', np.int32), ('column3', np.float64), ('column3', str)]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "server = \"localhost:9092\"\n",
    "topic = \"my_test_topic_1\"\n",
    "group_id = \"group1\"\n",
    "\n",
    "\n",
    "# Classification with known datatypes\n",
    "\n",
    "my_kafka_stream = KafkaStream(\n",
    "            dtypes = [('period', np.float64), ('nswprice', np.float64), ('nswdemand', np.float64), ('vicprice', np.float64), ('vicdemand', np.float64), ('transfer', np.float64)],\n",
    "            target_index = -1,\n",
    "            class_labels = ['0', '1'], # For classification tasks\n",
    "            server = server,\n",
    "            topic = topic, \n",
    "            group_id = group_id,\n",
    "            buffer_size = 3,\n",
    "            schema = None)\n",
    "\n",
    "\n",
    "# Classification with unknown datatypes\n",
    "\n",
    "my_kafka_stream = KafkaStream(\n",
    "            dtypes = None,\n",
    "            target_type = 'categorical', # \"numeric\" for regression, 'categorical' for classification\n",
    "            target_index = -1,\n",
    "            class_labels = ['0', '1'], # For classification tasks\n",
    "            server = server,\n",
    "            topic = topic, \n",
    "            group_id = group_id,\n",
    "            buffer_size = 3,\n",
    "            schema = None)\n",
    "\n",
    "\n",
    "# Regression with known datatypes\n",
    "\n",
    "my_kafka_stream = KafkaStream(\n",
    "            dtypes = [('period', np.float64), ('nswprice', np.float64), ('nswdemand', np.float64), ('vicprice', np.float64), ('vicdemand', np.float64), ('transfer', np.float64)], # [('column1', np.float64), ('column2', np.int32), ('column3', np.float64), ('column3', str)],\n",
    "            target_type = 'numeric', # \"numeric\" for regression, 'categorical' for classification\n",
    "            target_index = -1,\n",
    "            server = server,\n",
    "            topic = topic, \n",
    "            group_id = group_id,\n",
    "            buffer_size = 3,\n",
    "            schema = None)\n",
    "\n",
    "\n",
    "# Regression with unknown datatypes\n",
    "\n",
    "my_kafka_stream = KafkaStream(\n",
    "            dtypes = None,\n",
    "            target_type = 'numeric', # \"numeric\" for regression, 'categorical' for classification\n",
    "            target_index = -1,\n",
    "            server = server,\n",
    "            topic = topic, \n",
    "            group_id = group_id,\n",
    "            buffer_size = 3,\n",
    "            schema = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Operating the **KafkaStream** instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Poll Instances from Kafka Producer\n",
    "\n",
    "The **KafkaStream** works on a buffer system whose size is specified in the constructor using `buffer_size`. To load instances into this buffer we use the `poll_kafka()` method. Only after polling `next_instance()` can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "my_kafka_stream.poll_kafka()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get the next instance from the buffer.\n",
    "This method only works if the buffer is not empty; it raises an error otherwise. The instance can be stored in a local variable for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "my_kafka_stream.next_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Check if there are more instances in the current buffer\n",
    "\n",
    "This method checks if all the items in the current buffer have been processed or not. If `False' is returned, then onw would have to poll for any new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "my_kafka_stream.has_more_instances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Return the number of processed instances till now by the stream\n",
    "\n",
    "A processed instance is considered when it is returned by `next_instance()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "my_kafka_stream.get_processed_instances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Getter Methods\n",
    "\n",
    "- Schema\n",
    "- Buffer Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "my_kafka_stream.get_schema() # Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "my_kafka_stream.get_buffer_size() # Buffer Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Close the Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "my_kafka_stream.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
